{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week-01: Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/workspaces/mlops-zoomcamp/02-experiment_tracking/mlruns/1', creation_time=1716290291829, experiment_id='1', last_update_time=1716290291829, lifecycle_stage='active', name='nyc-taxi-experiment', tags={}>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading and loading the data\n",
    "We'll use the NYC taxi dataset, we'll use \"Yellow Taxi Trip Records\".\n",
    "\n",
    "Downloading the data for January and February 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_filepath = \"/workspaces/mlops-zoomcamp/data/green_tripdata_2023-01.parquet\"\n",
    "jan_df = pd.read_parquet(jan_filepath)\n",
    "\n",
    "feb_filepath = \"/workspaces/mlops-zoomcamp/data/green_tripdata_2023-02.parquet\"\n",
    "feb_df = pd.read_parquet(feb_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(jan_df.columns), jan_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing duration\n",
    "Let's compute the duration variable. It should contain the duration of a ride in minutes.\n",
    "`duration = tpep_dropoff_datetime - tpep_pickup_datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDuration(df):\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "\n",
    "    df[\"duration\"] = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_df = calculateDuration(jan_df)\n",
    "feb_df = calculateDuration(feb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_df.duration.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_df.duration.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dropping outliers\n",
    "Next, let's check the distribution of the duration variable. There are some outliers. Let's remove them and keep only the records where the `duration` was between 1 and 60 minutes (inclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers_by_duration(df, minD, maxD):\n",
    "    print(\"Fraction left: \",((df.duration >= minD) & (df.duration <= maxD)).mean())\n",
    "    return df[(df.duration >= minD) & (df.duration <= maxD)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction left:  0.9667942120772309\n",
      "Fraction left:  0.9655140489746794\n"
     ]
    }
   ],
   "source": [
    "jan_df = drop_outliers_by_duration(jan_df, 1, 60)\n",
    "feb_df = drop_outliers_by_duration(feb_df, 1, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One-hot encoding\n",
    "Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.\n",
    "\n",
    "- Turn the dataframe into a list of dictionaries (remember to re-cast the ids to strings - otherwise it will label encode them)\n",
    "- Fit a dictionary vectorizer\n",
    "- Get a feature matrix from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_locationID_toStr(df):\n",
    "    df[\"PULocationID\"] = df[\"PULocationID\"].astype(str)\n",
    "    df[\"DOLocationID\"] = df[\"DOLocationID\"].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_df = transform_locationID_toStr(jan_df)\n",
    "feb_df = transform_locationID_toStr(feb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\"PULocationID\", \"DOLocationID\"]\n",
    "numerical = []\n",
    "target = \"duration\"\n",
    "def train_val_data(train_df, val_df):\n",
    "    train_dicts = train_df[categorical + numerical].to_dict(orient=\"records\")\n",
    "    val_dicts = val_df[categorical+numerical].to_dict(orient=\"records\")\n",
    "    \n",
    "    dv = DictVectorizer()\n",
    "    \n",
    "    x_train = dv.fit_transform(train_dicts)\n",
    "    print(\"Dimensionality of feature matrix:\", x_train.shape)\n",
    "    x_val = dv.transform(val_dicts)\n",
    "\n",
    "    y_train = train_df[target].values\n",
    "    y_val = val_df[target].values\n",
    "    return x_train, y_train, x_val, y_val, dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of feature matrix: (65946, 467)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val, dv = train_val_data(train_df=jan_df, val_df=feb_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a model\n",
    "Let's use the feature matrix from the previous step to train a model.\n",
    "\n",
    "- Train a plain linear regression model with default parameters\n",
    "- Calculate the RMSE of the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_on_train = model.predict(x_train)\n",
    "print(\"RMSE on train: \",mean_squared_error(y_pred=y_pred_on_train, y_true=y_train, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_on_val = model.predict(x_val)\n",
    "print(\"RMSE on validation: \",mean_squared_error(y_pred=y_pred_on_val, y_true=y_val, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving the model\n",
    "Finally, let's use pickle and store our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/workspaces/mlops-zoomcamp/models/lin_reg.bin\", 'wb') as f_out:\n",
    "    pickle.dump(model, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    mlflow.set_tag(\"developer\", \"denil\")\n",
    "    mlflow.log_param(\"train-data-path\", jan_filepath)\n",
    "    mlflow.log_param(\"valid-data-path\", feb_filepath)\n",
    "\n",
    "    alpha = 0.1\n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "    model = Lasso(alpha)\n",
    "    model.fit(x_train,y_train)\n",
    "    y_pred_on_train = model.predict(x_train)\n",
    "    rmse = mean_squared_error(y_pred=y_pred_on_train, y_true=y_train, squared=False)\n",
    "\n",
    "    mlflow.log_metric(\"rmse \", rmse)\n",
    "    mlflow.log_artifact(local_path=\"../models/lin_reg.bin\", artifact_path=\"models_pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = xgb.DMatrix(x_train, label=y_train)\n",
    "valid = xgb.DMatrix(x_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")\n",
    "        mlflow.log_params(params)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=train,\n",
    "            num_boost_round=100,\n",
    "            evals=[(valid, \"validation\")],\n",
    "            early_stopping_rounds=5\n",
    "        )\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'reg:linear',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=Trials()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 0.3911485255644647,\n",
    "    \"max_depth\": 96,\n",
    "    \"min_child_weight\": 2.438960793020336,\n",
    "    \"objective\":\"reg:linear\",\n",
    "    \"reg_alpha\": 0.2924504415949103,\n",
    "    \"reg_lambda\": 0.27323946210032624,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "mlflow.xgboost.autolog()\n",
    "\n",
    "booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=train,\n",
    "            num_boost_round=100,\n",
    "            evals=[(valid, \"validation\")],\n",
    "            early_stopping_rounds=5\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-rmse:7.53941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [10:02:32] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation-rmse:6.76976\n",
      "[2]\tvalidation-rmse:6.38174\n",
      "[3]\tvalidation-rmse:6.20876\n",
      "[4]\tvalidation-rmse:6.09826\n",
      "[5]\tvalidation-rmse:6.05714\n",
      "[6]\tvalidation-rmse:6.03301\n",
      "[7]\tvalidation-rmse:6.01642\n",
      "[8]\tvalidation-rmse:6.00375\n",
      "[9]\tvalidation-rmse:5.98911\n",
      "[10]\tvalidation-rmse:5.98233\n",
      "[11]\tvalidation-rmse:5.96332\n",
      "[12]\tvalidation-rmse:5.96114\n",
      "[13]\tvalidation-rmse:5.95414\n",
      "[14]\tvalidation-rmse:5.95319\n",
      "[15]\tvalidation-rmse:5.94962\n",
      "[16]\tvalidation-rmse:5.94734\n",
      "[17]\tvalidation-rmse:5.94816\n",
      "[18]\tvalidation-rmse:5.94836\n",
      "[19]\tvalidation-rmse:5.94544\n",
      "[20]\tvalidation-rmse:5.94587\n",
      "[21]\tvalidation-rmse:5.94363\n",
      "[22]\tvalidation-rmse:5.94250\n",
      "[23]\tvalidation-rmse:5.94111\n",
      "[24]\tvalidation-rmse:5.93724\n",
      "[25]\tvalidation-rmse:5.93672\n",
      "[26]\tvalidation-rmse:5.93496\n",
      "[27]\tvalidation-rmse:5.93406\n",
      "[28]\tvalidation-rmse:5.93299\n",
      "[29]\tvalidation-rmse:5.93292\n",
      "[30]\tvalidation-rmse:5.93018\n",
      "[31]\tvalidation-rmse:5.93052\n",
      "[32]\tvalidation-rmse:5.93069\n",
      "[33]\tvalidation-rmse:5.92902\n",
      "[34]\tvalidation-rmse:5.92751\n",
      "[35]\tvalidation-rmse:5.92905\n",
      "[36]\tvalidation-rmse:5.92778\n",
      "[37]\tvalidation-rmse:5.92348\n",
      "[38]\tvalidation-rmse:5.92241\n",
      "[39]\tvalidation-rmse:5.91871\n",
      "[40]\tvalidation-rmse:5.91962\n",
      "[41]\tvalidation-rmse:5.91726\n",
      "[42]\tvalidation-rmse:5.91663\n",
      "[43]\tvalidation-rmse:5.91393\n",
      "[44]\tvalidation-rmse:5.91363\n",
      "[45]\tvalidation-rmse:5.91503\n",
      "[46]\tvalidation-rmse:5.91292\n",
      "[47]\tvalidation-rmse:5.91483\n",
      "[48]\tvalidation-rmse:5.91173\n",
      "[49]\tvalidation-rmse:5.91196\n",
      "[50]\tvalidation-rmse:5.91051\n",
      "[51]\tvalidation-rmse:5.90679\n",
      "[52]\tvalidation-rmse:5.90548\n",
      "[53]\tvalidation-rmse:5.90476\n",
      "[54]\tvalidation-rmse:5.90346\n",
      "[55]\tvalidation-rmse:5.90585\n",
      "[56]\tvalidation-rmse:5.90534\n",
      "[57]\tvalidation-rmse:5.90209\n",
      "[58]\tvalidation-rmse:5.90002\n",
      "[59]\tvalidation-rmse:5.89862\n",
      "[60]\tvalidation-rmse:5.89509\n",
      "[61]\tvalidation-rmse:5.89459\n",
      "[62]\tvalidation-rmse:5.89267\n",
      "[63]\tvalidation-rmse:5.89260\n",
      "[64]\tvalidation-rmse:5.89198\n",
      "[65]\tvalidation-rmse:5.89255\n",
      "[66]\tvalidation-rmse:5.88927\n",
      "[67]\tvalidation-rmse:5.88821\n",
      "[68]\tvalidation-rmse:5.88468\n",
      "[69]\tvalidation-rmse:5.88480\n",
      "[70]\tvalidation-rmse:5.88501\n",
      "[71]\tvalidation-rmse:5.88399\n",
      "[72]\tvalidation-rmse:5.88227\n",
      "[73]\tvalidation-rmse:5.88312\n",
      "[74]\tvalidation-rmse:5.88149\n",
      "[75]\tvalidation-rmse:5.88116\n",
      "[76]\tvalidation-rmse:5.88109\n",
      "[77]\tvalidation-rmse:5.88040\n",
      "[78]\tvalidation-rmse:5.87983\n",
      "[79]\tvalidation-rmse:5.87980\n",
      "[80]\tvalidation-rmse:5.87895\n",
      "[81]\tvalidation-rmse:5.87931\n",
      "[82]\tvalidation-rmse:5.87863\n",
      "[83]\tvalidation-rmse:5.87718\n",
      "[84]\tvalidation-rmse:5.87628\n",
      "[85]\tvalidation-rmse:5.87485\n",
      "[86]\tvalidation-rmse:5.87372\n",
      "[87]\tvalidation-rmse:5.87324\n",
      "[88]\tvalidation-rmse:5.87419\n",
      "[89]\tvalidation-rmse:5.87393\n",
      "[90]\tvalidation-rmse:5.87434\n",
      "[91]\tvalidation-rmse:5.87449\n",
      "[92]\tvalidation-rmse:5.86965\n",
      "[93]\tvalidation-rmse:5.86963\n",
      "[94]\tvalidation-rmse:5.86796\n",
      "[95]\tvalidation-rmse:5.86760\n",
      "[96]\tvalidation-rmse:5.86733\n",
      "[97]\tvalidation-rmse:5.86711\n",
      "[98]\tvalidation-rmse:5.86648\n",
      "[99]\tvalidation-rmse:5.86570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/23 10:02:43 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "2024/05/23 10:02:43 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/home/codespace/anaconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [10:02:43] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\"\n",
      "/home/codespace/anaconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [10:02:47] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with mlflow.start_run():\n",
    "    params = {\n",
    "        \"learning_rate\": 0.3911485255644647,\n",
    "        \"max_depth\": 96,\n",
    "        \"min_child_weight\": 2.438960793020336,\n",
    "        \"objective\":\"reg:linear\",\n",
    "        \"reg_alpha\": 0.2924504415949103,\n",
    "        \"reg_lambda\": 0.27323946210032624,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    booster = xgb.train(\n",
    "                params=params,\n",
    "                dtrain=train,\n",
    "                num_boost_round=100,\n",
    "                evals=[(valid, \"validation\")],\n",
    "                early_stopping_rounds=5\n",
    "            )\n",
    "    y_pred = booster.predict(valid)\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    with open(\"models/preprocessor.b\", 'wb') as f_out:\n",
    "        pickle.dump(dv, f_out)\n",
    "\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"models/preprocessor.b\",artifact_path=\"preprocessor\")\n",
    "    mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MLflowClient\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "\n",
    "client = MLflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_experiment(name=\"my-cool-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.entities import ViewType\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=\"1\",\n",
    "    filter_string=\"metrics.rmse < 6.8\",\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=5,\n",
    "    order_by=[\"metrics.rmse ASC\"]\n",
    ")\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"run id: {run.info.run_id}, rmse: {run.data.metrics['rmse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "run_id = \"\"\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "mlflow.register_model(model_uri=model_uri, name=\"nyc-taxi-regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"\"\n",
    "latest_versions = client.get_latest_versions(name=model_name)\n",
    "\n",
    "for version in latest_versions:\n",
    "    print(f\"version: {version.version}, stage: {version.current_stage}\")\n",
    "\n",
    "new_stage = \"Staging\"\n",
    "model_version = 4\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    stage=new_stage,\n",
    "    archive_existing_versions=False\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "date = datetime.today().date()\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=model_version,\n",
    "    description=f\"the model version {model_version} was transitioned to {new_stage} on {date}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
